{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder to fill in gene data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, add\n",
    "from keras.layers.core import Layer, Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading CSV input...\n",
      "df(3143,184)\n",
      "                 0        1    2         3        4         5         6    7  \\\n",
      "0  ENSG00000000419   0.0000  0.0   7.80771   0.0000   0.00000   0.00000  0.0   \n",
      "1  ENSG00000002586  24.6584  0.0  55.47970  44.1613   0.00000   4.83525  0.0   \n",
      "2  ENSG00000002834   0.0000  0.0   0.00000   0.0000   8.54906   9.77374  0.0   \n",
      "3  ENSG00000003056  78.2393  0.0  72.54200   0.0000  70.38990   0.00000  0.0   \n",
      "4  ENSG00000003402  12.9291  0.0   0.00000  12.8144   5.47288  22.74410  0.0   \n",
      "\n",
      "         8        9       ...         174  175  176  177  178  179       180  \\\n",
      "0  204.495   0.0000       ...         0.0  0.0  0.0  0.0  0.0  0.0    0.0000   \n",
      "1    0.000  95.1674       ...         0.0  0.0  0.0  0.0  0.0  0.0  133.0920   \n",
      "2    0.000   0.0000       ...         0.0  0.0  0.0  0.0  0.0  0.0   17.1774   \n",
      "3    0.000  55.4920       ...         0.0  0.0  0.0  0.0  0.0  0.0   74.4863   \n",
      "4    0.000  25.3042       ...         0.0  0.0  0.0  0.0  0.0  0.0    1.7521   \n",
      "\n",
      "        181  zero_each_row  zero_percentage  \n",
      "0   0.00000            132         0.721311  \n",
      "1   0.00000             84         0.459016  \n",
      "2   6.29613            137         0.748634  \n",
      "3  23.70210            127         0.693989  \n",
      "4   0.00000             99         0.540984  \n",
      "\n",
      "[5 rows x 184 columns]\n",
      "finished reading data...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "\n",
    "now = time.time()\n",
    "tag = str(now)\n",
    "\n",
    "DEBUG = True   # always\n",
    "LEARNING_RATE = 5e-6\n",
    "TRAINING_ITERATIONS = 20000\n",
    "\n",
    "# sizes of the hidden layers\n",
    "NN_HL_ARCH = [100, 100, 100, 50]  \n",
    "   \n",
    "# probability of keeping a neuron = 1-prob(dropout)\n",
    "DROPOUT = 1.0\n",
    "\n",
    "# batch size \n",
    "BATCH_SIZE = 1 # there may be some errors if set to > 1. \n",
    "\n",
    "# set to 0 to train on all available data\n",
    "# (currently validation is not used)\n",
    "VALIDATION_SIZE = 0 #2000\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    print('reading CSV input...')\n",
    "df = pd.read_csv('/home/danni/Dropbox/CS/Code/gene_autoencoder/zeros_removed.csv')\n",
    "\n",
    "print('df({0[0]},{0[1]})'.format(df.shape))\n",
    "print (df.head())\n",
    "\n",
    "if DEBUG:\n",
    "    print('finished reading data...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80% training set and 20% testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliting into training and testing dataset...\n",
      "finished splitting.\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    print('spliting into training and testing dataset...')\n",
    "# df['split'] = np.random.randn(df.shape[0], 1)\n",
    "\n",
    "# msk = np.random.rand(len(df)) <= 0.8\n",
    "\n",
    "# train = df[msk]\n",
    "# test = df[~msk]\n",
    "\n",
    "validation_inputs = inputs[:VALIDATION_SIZE]\n",
    "validation_labels = inputs[:VALIDATION_SIZE]\n",
    "\n",
    "train_inputs = inputs[VALIDATION_SIZE:]\n",
    "train_labels = inputs[VALIDATION_SIZE:]\n",
    "\n",
    "input_size = len(inputs[0])\n",
    "if DEBUG:\n",
    "    print('finished splitting.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating placeholders for input and output...\n"
     ]
    }
   ],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "if DEBUG:\n",
    "    print('creating placeholders for input and output...')\n",
    "    \n",
    "# inputs\n",
    "x = tf.placeholder('float', shape=[None,input_size])\n",
    "# outputs = labels\n",
    "y_ = tf.placeholder('float', shape=[None,input_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prevent Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating W,b,h variables for various layers...\n",
      "\tnow creating input layer...\n",
      "\t\t 183 , 100\n",
      "\tnow creating layer: 1\n",
      "\t\t 100 , 100\n",
      "\tnow creating layer: 2\n",
      "\t\t 100 , 100\n",
      "\tnow creating layer: 3\n",
      "\t\t 100 , 50\n",
      "\tnow creating output layer...\n",
      "\t\t 50 , 183\n",
      "\tsetting up output vector...\n"
     ]
    }
   ],
   "source": [
    "keep_prob = tf.placeholder('float')\n",
    "if DEBUG:\n",
    "    print('creating W,b,h variables for various layers...')\n",
    "    \n",
    "W = []\n",
    "b = []\n",
    "h = []\n",
    "h_tmp = []\n",
    "\n",
    "if DEBUG:\n",
    "    print('\\tnow creating input layer...')\n",
    "print('\\t\\t',input_size,',',NN_HL_ARCH[0])\n",
    "\n",
    "W.append(weight_variable([input_size,NN_HL_ARCH[0]]))\n",
    "b.append(bias_variable([NN_HL_ARCH[0]]))\n",
    "h.append(tf.nn.relu(tf.matmul(x,W[0]) + b[0]))\n",
    "\n",
    "for i in range(1,len(NN_HL_ARCH)):\n",
    "    u = NN_HL_ARCH[i-1]\n",
    "    v = NN_HL_ARCH[i]\n",
    "    if DEBUG:\n",
    "        print('\\tnow creating layer:', i)\n",
    "        print('\\t\\t',u,',',v)\n",
    "\n",
    "    W.append(weight_variable([u,v]))\n",
    "    b.append(bias_variable([v]))\n",
    "    if DEBUG:\n",
    "        pass\n",
    "        #print('h', tf.shape(h[i-1]), ', W:', tf.shape(W[i]), ', b:', tf.shape(b[i]))\n",
    "\n",
    "    h_tmp.append(tf.nn.relu(tf.matmul(h[i-1],W[i]) + b[i]))\n",
    "    # dropout\n",
    "    h.append(tf.nn.dropout(h_tmp[-1], keep_prob))\n",
    "\n",
    "if DEBUG:\n",
    "    print('\\tnow creating output layer...')\n",
    "    print('\\t\\t',NN_HL_ARCH[-1],',',input_size)\n",
    "\n",
    "W.append(weight_variable([NN_HL_ARCH[-1],input_size]))\n",
    "b.append(bias_variable([input_size]))\n",
    "if DEBUG:\n",
    "    print('\\tsetting up output vector...')\n",
    "y = tf.nn.relu(tf.matmul(h[-1],W[-1]) + b[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost function\n",
    "\n",
    "use gradiant based optimization algorithm: ADAM optimizaer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defining objective function...\n",
      "defining optimization step...\n",
      "defining optimization step...\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "if DEBUG:\n",
    "    print('defining objective function...')\n",
    "# but we shall use rmse\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.pow(y-y_, 2)))\n",
    "\n",
    "if DEBUG:\n",
    "    print('defining optimization step...')\n",
    "# optimisation function\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(rmse)\n",
    "\n",
    "# evaluation\n",
    "if DEBUG:\n",
    "    print('defining optimization step...')\n",
    "\n",
    "# CHECK\n",
    "error_sq_vector = tf.pow(y - y_,2)\n",
    "\n",
    "# CHECK\n",
    "accuracy = tf.sqrt(tf.reduce_mean(error_sq_vector))\n",
    "predict = tf.identity(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use small batches first to avoid huge cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = train_inputs.shape[0]\n",
    "\n",
    "# serve data by batches\n",
    "def next_batch(batch_size):\n",
    "    \n",
    "    global train_inputs\n",
    "    global train_labels\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # when all trainig data have been already used, it is reorder randomly    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        train_inputs = train_inputs[perm]\n",
    "        train_labels = train_labels[perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return train_inputs[start:end], train_labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy => 45.7149 for step 0\n",
      "training_accuracy => 87.1401 for step 1\n",
      "training_accuracy => 21.0431 for step 2\n",
      "training_accuracy => 55.4242 for step 3\n",
      "training_accuracy => 29.0490 for step 4\n",
      "training_accuracy => 30.0902 for step 5\n",
      "training_accuracy => 69.0275 for step 6\n",
      "training_accuracy => 638.4905 for step 7\n",
      "training_accuracy => 20.6792 for step 8\n",
      "training_accuracy => 59.5783 for step 9\n",
      "training_accuracy => 111.9487 for step 10\n",
      "training_accuracy => 13.8301 for step 20\n",
      "training_accuracy => 39.0512 for step 30\n",
      "training_accuracy => 21891.3125 for step 40\n",
      "training_accuracy => 518.8159 for step 50\n",
      "training_accuracy => 52.6602 for step 60\n",
      "training_accuracy => 30.6682 for step 70\n",
      "training_accuracy => 23.8834 for step 80\n",
      "training_accuracy => 155.1471 for step 90\n",
      "training_accuracy => 36.1892 for step 100\n",
      "training_accuracy => 49.4693 for step 200\n",
      "training_accuracy => 151.5613 for step 300\n",
      "training_accuracy => 74.5648 for step 400\n",
      "training_accuracy => 43.7708 for step 500\n",
      "training_accuracy => 118.8007 for step 600\n",
      "training_accuracy => 29.8585 for step 700\n",
      "training_accuracy => 41.2882 for step 800\n",
      "training_accuracy => 86.5128 for step 900\n",
      "training_accuracy => 20.9132 for step 1000\n",
      "training_accuracy => 27.1872 for step 2000\n",
      "training_accuracy => 35.5429 for step 3000\n",
      "training_accuracy => 33.6764 for step 4000\n",
      "training_accuracy => 30.4103 for step 5000\n",
      "training_accuracy => 24.8755 for step 6000\n",
      "training_accuracy => 80.8357 for step 7000\n",
      "training_accuracy => 46.7324 for step 8000\n",
      "training_accuracy => 22.6934 for step 9000\n",
      "training_accuracy => 27.1144 for step 10000\n",
      "training_accuracy => 41.2186 for step 19999\n"
     ]
    }
   ],
   "source": [
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "x_range = []\n",
    "\n",
    "display_step=1\n",
    "\n",
    "for i in range(TRAINING_ITERATIONS):\n",
    "\n",
    "    #get new batch\n",
    "    batch_xs, batch_ys = next_batch(BATCH_SIZE)        \n",
    "\n",
    "    # check progress on every 1st,2nd,...,10th,20th,...,100th... step\n",
    "    if i%display_step == 0 or (i+1) == TRAINING_ITERATIONS:\n",
    "        \n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_xs, \n",
    "                                                  y_: batch_ys, \n",
    "                                                  keep_prob: 1.0})       \n",
    "        if(VALIDATION_SIZE):\n",
    "            validation_accuracy = accuracy.eval(feed_dict={ x: validation_inputs[0:BATCH_SIZE], \n",
    "                                                            y_: validation_labels[0:BATCH_SIZE], \n",
    "                                                            keep_prob: 1.0})                                  \n",
    "            print('training_accuracy / validation_accuracy => %.2f / %.2f for step %d'%(train_accuracy, validation_accuracy, i))\n",
    "            \n",
    "            validation_accuracies.append(validation_accuracy)\n",
    "            \n",
    "        else:\n",
    "             print('training_accuracy => %.4f for step %d'%(train_accuracy, i))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        x_range.append(i)\n",
    "        \n",
    "        # increase display_step\n",
    "        if i%(display_step*10) == 0 and i:\n",
    "            display_step *= 10\n",
    "    # train on batch\n",
    "sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: DROPOUT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(VALIDATION_SIZE):\n",
    "    validation_accuracy = accuracy.eval(feed_dict={x: validation_inputs, \n",
    "                                                   y_: validation_labels, \n",
    "                                                   keep_prob: 1.0})\n",
    "    print('validation_accuracy => %.4f'%validation_accuracy)\n",
    "    plt.plot(x_range, train_accuracies,'-b', label='Training')\n",
    "    plt.plot(x_range, validation_accuracies,'-g', label='Validation')\n",
    "    plt.legend(loc='lower right', frameon=False)\n",
    "    plt.ylim(ymax = 1.1, ymin = 0.7)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('step')\n",
    "    plt.show()\n",
    "    print('finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
